{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5ClQFEGeW3+uer3OdKUS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryane-jon/Natural-Language-Processing-Coursework/blob/main/NLP_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Proccessing (CE4145) Coursework\n",
        "- Ryan Jones (2208751)\n",
        "- Dataset Source: [Sentiment Labelled Sentences](https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences)\n",
        "\n",
        "This dataset consists of:\n",
        "- **Sentences:** in the form of reviews sourced from Amazon, IMDB and Yelp.\n",
        "- **Sentiments:** represented as 1s for positive, and 0s for negative.  \n",
        "- **Sources:** as seperate files; Each file is data from a different source, each with 1000 different sentences.\n",
        "\n",
        "Tasks:\n",
        "- Create a model that can predict the sentiment (Positive or Negative) of various sentences.\n",
        "- I also plan to extend this to detect the source of these sentences (Amazon, imdb or yelp)"
      ],
      "metadata": {
        "id": "n-iRchoXFkET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset & Pre-Processing"
      ],
      "metadata": {
        "id": "HA5HB7Whshrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset\n",
        "(from the download of the dataset in the provided link)"
      ],
      "metadata": {
        "id": "OTJbcmqbkHay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 3 files are imported (amazon_cells_labelled.txt, imdb_labelled.txt, yelp_labelled.txt)"
      ],
      "metadata": {
        "id": "vU4XgwDOJM-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#used to upload data\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload() #files can be uploaded here, I select the 3 labelled files."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "w125YzERIq-D",
        "outputId": "b26fffc3-a456-4ace-e543-8b204ad9c411"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad8c8ff6-6a04-4c25-86d9-d7abf4f3c902\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad8c8ff6-6a04-4c25-86d9-d7abf4f3c902\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amazon_cells_labelled.txt to amazon_cells_labelled.txt\n",
            "Saving imdb_labelled.txt to imdb_labelled.txt\n",
            "Saving yelp_labelled.txt to yelp_labelled.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import stuff to use later\n",
        "from sklearn.pipeline import Pipeline #let's import the pipeline functionality\n",
        "from sklearn.feature_extraction.text import CountVectorizer #and we will import a simple pre-processing method\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #and a representation learner\n",
        "from sklearn.neighbors import KNeighborsClassifier #and a simple classifier model\n",
        "from sklearn.model_selection import StratifiedKFold #cross fold is sometimes called k-fold. Calling the stratified version ensures that classes have equal representation across folds\n",
        "from sklearn.metrics import accuracy_score #import an accuracy metric to tell us how well the model is doing"
      ],
      "metadata": {
        "id": "naT06_b4hQ8c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split each files lines into strings in arrays to be processed"
      ],
      "metadata": {
        "id": "Dd31d72_J_Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_data = uploaded['amazon_cells_labelled.txt'].decode('UTF-8').splitlines()\n",
        "imdb_data = uploaded['imdb_labelled.txt'].decode('UTF-8').splitlines()\n",
        "yelp_data = uploaded['yelp_labelled.txt'].decode('UTF-8').splitlines()\n",
        "\n",
        "#View some of the data\n",
        "for index in range(5):\n",
        "  print(amazon_data[index])\n",
        "  print(imdb_data[index])\n",
        "  print(yelp_data[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqTTBZDLJn5Q",
        "outputId": "a3959d1d-9026-48c6-b3c2-111ceba29adf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\n",
            "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \t0\n",
            "Wow... Loved this place.\t1\n",
            "Good case, Excellent value.\t1\n",
            "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \t0\n",
            "Crust is not good.\t0\n",
            "Great for the jawbone.\t1\n",
            "Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  \t0\n",
            "Not tasty and the texture was just nasty.\t0\n",
            "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\n",
            "Very little music or anything to speak of.  \t0\n",
            "Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\t1\n",
            "The mic is great.\t1\n",
            "The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  \t1\n",
            "The selection on the menu was great and so were the prices.\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a numpy array with 3 collumns: Review, Sentiment, and Source"
      ],
      "metadata": {
        "id": "LoCAsCOGes-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "reviews = []\n",
        "sentiments = []\n",
        "sources = []\n",
        "\n",
        "\n",
        "\n",
        "for line in amazon_data:\n",
        "  reviews.append(line[:-1])\n",
        "  sentiments.append(line[-1])\n",
        "  sources.append(\"Amazon\")\n",
        "\n",
        "for line in imdb_data:\n",
        "  reviews.append(line[:-1])\n",
        "  sentiments.append(line[-1])\n",
        "  sources.append(\"IMDB\")\n",
        "\n",
        "for line in yelp_data:\n",
        "  reviews.append(line[:-1])\n",
        "  sentiments.append(line[-1])\n",
        "  sources.append(\"Yelp\")\n",
        "\n",
        "\n",
        "data = np.column_stack((reviews, sentiments, sources))\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMY2romENwfo",
        "outputId": "8ef07ce3-a7a0-4bc7-f02d-23d1a187128e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3002, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate Imported Data"
      ],
      "metadata": {
        "id": "EGp-rusvx_Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checks through the sentiment values to make sure they're all 0 or 1\n",
        "def checkSentimentValid(datas, print_info=False): #datas is the numpy array, print_info turns on printing info\n",
        "  x,y,z = datas.T\n",
        "  markedForDeath = [] #contains bad eggs\n",
        "  count = 0\n",
        "  for sentiment in y: #for each sentiment value\n",
        "    if sentiment != \"0\" and sentiment != \"1\": # make sure it's \"0\" or \"1\", if not, get as relevant surrounding info\n",
        "      markedForDeath.append(count)  #an omen\n",
        "      if print_info:    #print as much context as possible\n",
        "        print(str(count))\n",
        "        print(x[count]+\"\\t\"+y[count]+\"\\t\"+z[count])\n",
        "        print(x[count+1]+\"\\t\"+y[count+1]+\"\\t\"+z[count+1]+\"\\n\")\n",
        "    count= count+1\n",
        "  if print_info and len(markedForDeath)==0:\n",
        "    print(\"No bad eggs found\")\n",
        "  return markedForDeath\n",
        "\n",
        "checkSentimentValid(data, True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_czsGhsjwSM4",
        "outputId": "cf033b7d-2999-460c-d61b-564b79ea9a55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1178\n",
            "The script i\ts\tIMDB\n",
            "was there a script?  \t\t0\tIMDB\n",
            "\n",
            "1968\n",
            "Definitely worth seein\tg\tIMDB\n",
            " it's the sort of thought provoking film that forces you to question your own threshold of loneliness.  \t\t1\tIMDB\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1178, 1968]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a result of some reviews containing a character that is misinterpereted as a new line, splitting the review in two. With this knowledge, I can fix these values."
      ],
      "metadata": {
        "id": "1O1dvdY7y6YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marked = checkSentimentValid(data) #store the bad eggs\n",
        "marked.sort(reverse=True) #Sorted in reverse so I can pop one data without changing the index of the other\n",
        "for marker in marked:\n",
        "  if sentiments[marker] != \"0\" and sentiments[marker] != \"1\": #Ensure the data is bad (in case I accidentally run this block again)\n",
        "    reviews[marker+1] = reviews[marker] + sentiments[marker]+\" \" + reviews[1969] #rejoin the split review as one\n",
        "    reviews.pop(marker) #remove the leftover data\n",
        "    sentiments.pop(marker)\n",
        "    sources.pop(marker)\n",
        "\n",
        "data = np.column_stack((reviews, sentiments, sources))  #redefine data"
      ],
      "metadata": {
        "id": "UdHvL792zZ6N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "should now be no issue"
      ],
      "metadata": {
        "id": "vOOEBQIu1WTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkSentimentValid(data, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HwhCDCR1Nzs",
        "outputId": "ec37e809-ebf1-4dba-a4a1-4d35b93bb4a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No bad eggs found\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Final np Array"
      ],
      "metadata": {
        "id": "2FSxfhP9IPQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_array = []\n",
        "for i in range(len(reviews)):\n",
        "  random_array.append(i)\n",
        "np.random.shuffle(random_array)\n",
        "\n",
        "shuffled_reviews = []\n",
        "shuffled_sentiments = []\n",
        "shuffled_sources = []\n",
        "\n",
        "for i in random_array:\n",
        "  shuffled_reviews.append(reviews[i])\n",
        "  shuffled_sentiments.append(sentiments[i])\n",
        "  shuffled_sources.append(sources[i])\n",
        "\n",
        "data = np.column_stack((shuffled_reviews, shuffled_sentiments, shuffled_sources))"
      ],
      "metadata": {
        "id": "JV6LSKYVH5rv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Prep in NLP Pipeline.\n",
        "- To start with, it's just predicting the sentiment based on the review text, I will bring the source in (possibly as an additional feature or as a target).\n",
        "- **This section is mostly me testing some pre-proccessing and tokenization methods to see what is actually helpful**"
      ],
      "metadata": {
        "id": "opzVPXiIe6HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, z = data.T  #x=reviews, y=sentiment, z=source\n",
        "text_clf = Pipeline([ #Pipeline to organise functions\n",
        "  ('prep', CountVectorizer()), #Count vectorizer\n",
        "  ('rep', TfidfTransformer()), #representation learning method using tf-idf\n",
        "  ('mod', KNeighborsClassifier()), #kNN classifier\n",
        "  ])\n",
        "\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "  text_clf.fit(x_train, y_train) #fit to training data\n",
        "  predictions = text_clf.predict(x_test) #predict on test data\n",
        "  acc_score.append(accuracy_score(predictions, y_test)) #get accuracy\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score)) #mean accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEFhpkcFeeoN",
        "outputId": "7bb67891-999c-4b6a-aa54-c2eb92c4cdb7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7723333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-checking that data is being represented correctly"
      ],
      "metadata": {
        "id": "6iOSlEGKh_Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(0,4):\n",
        "  print(x[index*100])\n",
        "  print(y[index*100])\n",
        "  print(z[index*100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziyUSCkkhihr",
        "outputId": "67f6b2ca-1566-42df-e2f5-2f23571113e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "However, there was so much garlic in the fondue, it was barely edible.\t\n",
            "0\n",
            "Yelp\n",
            "The WORST EXPERIENCE EVER.\t\n",
            "0\n",
            "Yelp\n",
            "The shrimp tender and moist.\t\n",
            "1\n",
            "Yelp\n",
            "It was a great phone.\t\n",
            "1\n",
            "Amazon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduce tokenization and more pre-proccessing techniques into the pipeline"
      ],
      "metadata": {
        "id": "8akqpSCfqeaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages for tokenization"
      ],
      "metadata": {
        "id": "DLOvwRKdfGoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #import the natural language toolkit\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szD7febWsFZK",
        "outputId": "98b3f41f-d445-4893-9b1c-c6f909600e68"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new tokenize function and add it to the pipeline"
      ],
      "metadata": {
        "id": "2Byp3Si8fPr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "class pre_process_tokenize(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      for x in X:\n",
        "            #basic tokenisation\n",
        "            token_text = word_tokenize(x)\n",
        "            #lower casing & punctiuation removal)\n",
        "            normd_text = [token.lower() for token in token_text]\n",
        "            #stopword removal\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] #list compression to remove any stopwords from our list\n",
        "            #stemming\n",
        "            stemmer = SnowballStemmer(\"english\") #specify we are using the English stemming rules, as other languages are present in toolkit\n",
        "            prep_text += [[stemmer.stem(word) for word in swr_text]] #list compression for applying the stemmer\n",
        "\n",
        "      #rejoin the sentences\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "      return prep_sentences\n",
        "\n",
        "text_clf = Pipeline([\n",
        "  ('prep', pre_process_tokenize()),\n",
        "  ('rep', TfidfVectorizer()),\n",
        "  ('mod', KNeighborsClassifier()),\n",
        "  ])\n",
        "\n",
        "\n",
        "acc_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "for train, test in kf.split(x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
        "\n",
        "  text_clf.fit(x_train, y_train)\n",
        "  predictions = text_clf.predict(x_test)\n",
        "  acc = accuracy_score(predictions, y_test)\n",
        "  acc_score.append(acc)\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZMQKN4OrLlg",
        "outputId": "8712b019-2656-4023-80f3-5188875d8b9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7396666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has decreased the overall accuracy.\n",
        "- It's possible that stemming and stop word removal caused context and nuance to be lost, which was made worse by the fact that the each data point is only a single sentence, coming from multiple different sources.\n",
        "- I.E Without the source to contextualise the review, the feature starts to lose some important meaning.\n",
        "- Additionally, punctiuation and capitilisation could be very important for determining sentiment. E.g. \"No. Price too high!\" vs \"no price too high\", or the one about uncle jack and the horse.\n",
        "\n"
      ],
      "metadata": {
        "id": "VZXHYRwzbA9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Upon testing various combinations of pre-proccessing techniques, the only one that improved accuracy from the base NLP pipeline was **negation handling**\n",
        "- The other techniques would probably work a lot better if the data points were more than just one sentence, but as it stands every bit of context is important for determining sentiment. That is why the prep function probably won't be incredibly complex."
      ],
      "metadata": {
        "id": "ZaxcoVVrqEI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create final prep function"
      ],
      "metadata": {
        "id": "2uhFaj_Afrp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only includes basic tokenisation and negation handling"
      ],
      "metadata": {
        "id": "8au1CsZof1ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def prep(X):\n",
        "  #define negation words (I used a lil AI for this bit)\n",
        "  negation_words = {\"not\", \"no\", \"never\", \"n't\", \"don't\", \"won't\", \"can't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"doesn't\", \"didn't\", \"couldn't\", \"shouldn't\", \"wouldn't\", \"hardly\", \"barely\", \"scarcely\", \"seldom\"}\n",
        "  punctuation_marks = set(string.punctuation)\n",
        "\n",
        "  prep_text = []\n",
        "  for x in X:\n",
        "        token_text = word_tokenize(x) #basic tokenization\n",
        "        processed_tokens = [] #used to store new tokens post-negation-handling\n",
        "        negation_active = False #used to tell us if the last word was a negation word, false to start with\n",
        "\n",
        "        for token in token_text:\n",
        "            normalized_token = token.lower()  #normalise the tokens to compare to the negation words\n",
        "\n",
        "            if normalized_token in negation_words:      #if token is a negation word (not, never etc)\n",
        "                negation_active = True                    #set negation to true for the next word\n",
        "                processed_tokens.append(token)            #append unchanged token\n",
        "            elif normalized_token in punctuation_marks: #if token is a punctuation mark (i.e a seperator of ideas)\n",
        "                negation_active = False                   #set negation to false\n",
        "                processed_tokens.append(token)            #append unchanged token\n",
        "            elif negation_active:                       #if the last word was a negation word\n",
        "                processed_tokens.append(token + '_NEG')   #append with a negation identifier\n",
        "            else:                                       #if it's just a normal word\n",
        "                processed_tokens.append(token)            #append unchanged token\n",
        "        prep_text.append(processed_tokens)\n",
        "\n",
        "  #rejoin the sentences\n",
        "  prep_sentences = [\" \".join(sentence) for sentence in prep_text]\n",
        "  return prep_sentences\n",
        "\n",
        "prep_x = prep(x)"
      ],
      "metadata": {
        "id": "a8okOppWZ8tL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representation Learning"
      ],
      "metadata": {
        "id": "IdL5anXzxAom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Possible techniques to use\n",
        "- Word2Vec\n",
        "  - Representation of contextual knowledge would be very helpful considering textual context has proven to be very important for this model.\n",
        "- Tfidf\n",
        "  - I could add n-grams to the rep I used in the test pipeline to make it more effective. Although, since I already added negation handling, N-grams may not make much difference.\n",
        "- Doc2Vec\n",
        "  - Representation of an entire sentence could catch a lot of nuance and context that would be missed by a word-by-word representation, it could take the workload off of the final algorithm. However, this depends on how accurate it is compared to the final applied algorithm.\n"
      ],
      "metadata": {
        "id": "XAgkAX8ExEFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "u5wmYipy7F9_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec_rep(sentence, w2v_model):\n",
        "  embs = [w2v_model.wv[word] for word in sentence if word in w2v_model.wv.index_to_key]\n",
        "  sent_emb = np.mean(np.array(embs), 0)\n",
        "  return sent_emb"
      ],
      "metadata": {
        "id": "fsBFsx5C7MJo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "knn_score = []\n",
        "mlp_score = []\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5)\n",
        "prepn_x = np.array(prep_x)\n",
        "\n",
        "for train, test in kf.split(prepn_x,y):\n",
        "\n",
        "  x_train, x_test, y_train, y_test = prepn_x[train], prepn_x[test], y[train], y[test]\n",
        "\n",
        "  w2v_model = Word2Vec(vector_size=300, window=5, min_count=3, workers=4)\n",
        "  w2v_model.build_vocab(x_train, update=None)\n",
        "  w2v_model.train(corpus_iterable=x_train, total_examples=len(x_train), epochs=10)\n",
        "\n",
        "  x_train_representations = [word2vec_rep(instance, w2v_model) for instance in x_train]\n",
        "  x_test_representations = [word2vec_rep(instance, w2v_model) for instance in x_test]\n",
        "\n",
        "  kNN = KNeighborsClassifier(n_neighbors=3)\n",
        "  kNN.fit(x_train_representations, y_train)\n",
        "  knn_predictions = kNN.predict(x_test_representations)\n",
        "  knn_acc = accuracy_score(knn_predictions, y_test)\n",
        "  knn_score.append(knn_acc)\n",
        "\n",
        "  mlp = MLPClassifier(alpha=1, max_iter=10)\n",
        "  mlp.fit(x_train_representations, y_train)\n",
        "  mlp_predictions = mlp.predict(x_test_representations)\n",
        "  mlp_acc = accuracy_score(mlp_predictions, y_test)\n",
        "  mlp_score.append(mlp_acc)\n",
        "\n",
        "print(\"kNN Accuracy:\", np.mean(knn_score), \" MLP Accuracy:\", np.mean(mlp_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UcwOvCR7P9w",
        "outputId": "446f3843-1e6c-4df6-c435-15b9655cf912"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'numpy.str_'>.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'numpy.str_'>.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'numpy.str_'>.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'numpy.str_'>.\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'numpy.str_'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN Accuracy: 0.578  MLP Accuracy: 0.6066666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}